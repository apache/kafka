# Use Cases

Here is a description of a few of the popular use cases for Apache
KafkaÂ®. For an overview of a number of these areas in action, see 
[this blog post](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/).

## Messaging {#uses_messaging .anchor-link}

Kafka works well as a replacement for a more traditional message broker.
Message brokers are used for a variety of reasons (to decouple
processing from data producers, to buffer unprocessed messages, etc). In
comparison to most messaging systems Kafka has better throughput,
built-in partitioning, replication, and fault-tolerance which makes it a
good solution for large scale message processing applications.

In our experience messaging uses are often comparatively low-throughput,
but may require low end-to-end latency and often depend on the strong
durability guarantees Kafka provides.

In this domain Kafka is comparable to traditional messaging systems such
as [ActiveMQ](http://activemq.apache.org) or [RabbitMQ](https://www.rabbitmq.com).

## Website Activity Tracking {#uses_website .anchor-link}

The original use case for Kafka was to be able to rebuild a user
activity tracking pipeline as a set of real-time publish-subscribe
feeds. This means site activity (page views, searches, or other actions
users may take) is published to central topics with one topic per
activity type. These feeds are available for subscription for a range of
use cases including real-time processing, real-time monitoring, and
loading into Hadoop or offline data warehousing systems for offline
processing and reporting.

Activity tracking is often very high volume as many activity messages
are generated for each user page view.

## Metrics {#uses_metrics .anchor-link}

Kafka is often used for operational monitoring data. This involves
aggregating statistics from distributed applications to produce
centralized feeds of operational data.

## Log Aggregation {#uses_logs .anchor-link}

Many people use Kafka as a replacement for a log aggregation solution.
Log aggregation typically collects physical log files off servers and
puts them in a central place (a file server or HDFS perhaps) for
processing. Kafka abstracts away the details of files and gives a
cleaner abstraction of log or event data as a stream of messages. This
allows for lower-latency processing and easier support for multiple data
sources and distributed data consumption. In comparison to log-centric
systems like Scribe or Flume, Kafka offers equally good performance,
stronger durability guarantees due to replication, and much lower
end-to-end latency.

## Stream Processing {#uses_streamprocessing .anchor-link}

Many users of Kafka process data in processing pipelines consisting of
multiple stages, where raw input data is consumed from Kafka topics and
then aggregated, enriched, or otherwise transformed into new topics for
further consumption or follow-up processing. For example, a processing
pipeline for recommending news articles might crawl article content from
RSS feeds and publish it to an \"articles\" topic; further processing
might normalize or deduplicate this content and publish the cleansed
article content to a new topic; a final processing stage might attempt
to recommend this content to users. Such processing pipelines create
graphs of real-time data flows based on the individual topics. Starting
in 0.10.0.0, a light-weight but powerful stream processing library
called [Kafka Streams](../streams) is available in Apache
Kafka to perform such data processing as described above. Apart from
Kafka Streams, alternative open source stream processing tools include
[Apache Storm](https://storm.apache.org/) and [Apache Samza](http://samza.apache.org/).

## Event Sourcing {#uses_eventsourcing .anchor-link}

[Event sourcing](http://martinfowler.com/eaaDev/EventSourcing.html) is a
style of application design where state changes are logged as a
time-ordered sequence of records. Kafka\'s support for very large stored
log data makes it an excellent backend for an application built in this
style.

## Commit Log {#uses_commitlog .anchor-link}

Kafka can serve as a kind of external commit-log for a distributed
system. The log helps replicate data between nodes and acts as a
re-syncing mechanism for failed nodes to restore their data. 
The [log compaction](../design#compaction) feature in Kafka helps support this usage. 
In this usage Kafka is similar to [Apache BookKeeper](https://bookkeeper.apache.org/) project.
