# Kafka protocol guide

This document covers the wire protocol implemented in Kafka. It is meant
to give a readable guide to the protocol that covers the available
requests, their binary format, and the proper way to make use of them to
implement a client. This document assumes you understand the basic
design and terminology described
[here](https://kafka.apache.org/documentation.html#design)

-   [Preliminaries](#protocol_preliminaries)
    -   [Network](#protocol_network)
    -   [Partitioning and bootstrapping](#protocol_partitioning)
    -   [Partitioning Strategies](#protocol_partitioning_strategies)
    -   [Batching](#protocol_batching)
    -   [Versioning and Compatibility](#protocol_compatibility)
    -   [Retrieving Supported API versions](#api_versions)
    -   [SASL Authentication Sequence](#sasl_handshake)
-   [The Protocol](#protocol_details)
    -   [Protocol Primitive Types](#protocol_types)
    -   [Notes on reading the request format grammars](#protocol_grammar)
    -   [Common Request and Response Structure](#protocol_common)
    -   [Record Batch](#protocol_recordbatch)
-   [Evolving the Protocol](#protocol_evolution) //TODO: broken link
    -   [The Request Header](#protocol_versioning) //TODO: broken link
    -   [Versioning](#protocol_versioning) //TODO: broken link
-   [Constants](#protocol_constants)
    -   [Error Codes](#protocol_error_codes)
    -   [Api Keys](#protocol_api_keys)
-   [The Messages](#protocol_messages)
-   [Some Common Philosophical Questions](#protocol_philosophy)

#### Preliminaries {#protocol_preliminaries .anchor-link}

##### Network {#protocol_network .anchor-link}

Kafka uses a binary protocol over TCP. The protocol defines all APIs as
request response message pairs. All messages are size delimited and are
made up of the following primitive types.

The client initiates a socket connection and then writes a sequence of
request messages and reads back the corresponding response message. No
handshake is required on connection or disconnection. TCP is happier if
you maintain persistent connections used for many requests to amortize
the cost of the TCP handshake, but beyond this penalty connecting is
pretty cheap.

The client will likely need to maintain a connection to multiple
brokers, as data is partitioned and the clients will need to talk to the
server that has their data. However it should not generally be necessary
to maintain multiple connections to a single broker from a single client
instance (i.e. connection pooling).

The server guarantees that on a single TCP connection, requests will be
processed in the order they are sent and responses will return in that
order as well. The broker\'s request processing allows only a single
in-flight request per connection in order to guarantee this ordering.
Note that clients can (and ideally should) use non-blocking IO to
implement request pipelining and achieve higher throughput. i.e.,
clients can send requests even while awaiting responses for preceding
requests since the outstanding requests will be buffered in the
underlying OS socket buffer. All requests are initiated by the client,
and result in a corresponding response message from the server except
where noted.

The server has a configurable maximum limit on request size and any
request that exceeds this limit will result in the socket being
disconnected.

##### Partitioning and bootstrapping {#protocol_partitioning .anchor-link}

Kafka is a partitioned system so not all servers have the complete data
set. Instead recall that topics are split into a pre-defined number of
partitions, P, and each partition is replicated with some replication
factor, N. Topic partitions themselves are just ordered \"commit logs\"
numbered 0, 1, \..., P-1.

All systems of this nature have the question of how a particular piece
of data is assigned to a particular partition. Kafka clients directly
control this assignment, the brokers themselves enforce no particular
semantics of which messages should be published to a particular
partition. Rather, to publish messages the client directly addresses
messages to a particular partition, and when fetching messages, fetches
from a particular partition. If two clients want to use the same
partitioning scheme they must use the same method to compute the mapping
of key to partition.

These requests to publish or fetch data must be sent to the broker that
is currently acting as the leader for a given partition. This condition
is enforced by the broker, so a request for a particular partition to
the wrong broker will result in an the NotLeaderForPartition error code
(described below).

How can the client find out which topics exist, what partitions they
have, and which brokers currently host those partitions so that it can
direct its requests to the right hosts? This information is dynamic, so
you can\'t just configure each client with some static mapping file.
Instead all Kafka brokers can answer a metadata request that describes
the current state of the cluster: what topics there are, which
partitions those topics have, which broker is the leader for those
partitions, and the host and port information for these brokers.

In other words, the client needs to somehow find one broker and that
broker will tell the client about all the other brokers that exist and
what partitions they host. This first broker may itself go down so the
best practice for a client implementation is to take a list of two or
three URLs to bootstrap from. The user can then choose to use a load
balancer or just statically configure two or three of their Kafka hosts
in the clients.

The client does not need to keep polling to see if the cluster has
changed; it can fetch metadata once when it is instantiated cache that
metadata until it receives an error indicating that the metadata is out
of date. This error can come in two forms: (1) a socket error indicating
the client cannot communicate with a particular broker, (2) an error
code in the response to a request indicating that this broker no longer
hosts the partition for which data was requested.

1.  Cycle through a list of \"bootstrap\" Kafka URLs until we find one
    we can connect to. Fetch cluster metadata.
2.  Process fetch or produce requests, directing them to the appropriate
    broker based on the topic/partitions they send to or fetch from.
3.  If we get an appropriate error, refresh the metadata and try again.

##### Partitioning Strategies {#protocol_partitioning_strategies .anchor-link}

As mentioned above the assignment of messages to partitions is something
the producing client controls. That said, how should this functionality
be exposed to the end-user?

Partitioning really serves two purposes in Kafka:

1.  It balances data and request load over brokers
2.  It serves as a way to divvy up processing among consumer processes
    while allowing local state and preserving order within the
    partition. We call this semantic partitioning.

For a given use case you may care about only one of these or both.

To accomplish simple load balancing a simple approach would be for the
client to just round robin requests over all brokers. Another
alternative, in an environment where there are many more producers than
brokers, would be to have each client chose a single partition at random
and publish to that. This later strategy will result in far fewer TCP
connections.

Semantic partitioning means using some key in the message to assign
messages to partitions. For example if you were processing a click
message stream you might want to partition the stream by the user id so
that all data for a particular user would go to a single consumer. To
accomplish this the client can take a key associated with the message
and use some hash of this key to choose the partition to which to
deliver the message.

##### Batching {#protocol_batching .anchor-link}

Our APIs encourage batching small things together for efficiency. We
have found this is a very significant performance win. Both our API to
send messages and our API to fetch messages always work with a sequence
of messages not a single message to encourage this. A clever client can
make use of this and support an \"asynchronous\" mode in which it
batches together messages sent individually and sends them in larger
clumps. We go even further with this and allow the batching across
multiple topics and partitions, so a produce request may contain data to
append to many partitions and a fetch request may pull data from many
partitions all at once.

The client implementer can choose to ignore this and send everything one
at a time if they like.

##### Compatibility {#protocol_compatibility .anchor-link}

Kafka has a \"bidirectional\" client compatibility policy. In other
words, new clients can talk to old servers, and old clients can talk to
new servers. This allows users to upgrade either clients or servers
without experiencing any downtime.

Since the Kafka protocol has changed over time, clients and servers need
to agree on the schema of the message that they are sending over the
wire. This is done through API versioning.

Before each request is sent, the client sends the API key and the API
version. These two 16-bit numbers, when taken together, uniquely
identify the schema of the message to follow.

The intention is that clients will support a range of API versions. When
communicating with a particular broker, a given client should use the
highest API version supported by both and indicate this version in their
requests.

The server will reject requests with a version it does not support, and
will always respond to the client with exactly the protocol format it
expects based on the version it included in its request. The intended
upgrade path is that new features would first be rolled out on the
server (with the older clients not making use of them) and then as newer
clients are deployed these new features would gradually be taken
advantage of.

Note that [KIP-482 tagged fields](https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields)
can be added to a request without incrementing the version number. This
offers an additional way of evolving the message schema without breaking
compatibility. Tagged fields do not take up any space when the field is
not set. Therefore, if a field is rarely used, it is more efficient to
make it a tagged field than to put it in the mandatory schema. However,
tagged fields are ignored by recipients that don\'t know about them,
which could pose a challenge if this is not the behavior that the sender
wants. In such cases, a version bump may be more appropriate.

##### Retrieving Supported API versions {#api_versions .anchor-link}

In order to work against multiple broker versions, clients need to know
what versions of various APIs a broker supports. The broker exposes this
information since 0.10.0.0 as described in
[KIP-35](https://cwiki.apache.org/confluence/display/KAFKA/KIP-35+-+Retrieving+protocol+version).
Clients should use the supported API versions information to choose the
highest API version supported by both client and broker. If no such
version exists, an error should be reported to the user.

The following sequence may be used by a client to obtain supported API
versions from a broker.

1.  Client sends `ApiVersionsRequest` to a broker after connection has
    been established with the broker. If SSL is enabled, this happens
    after SSL connection has been established.
2.  On receiving `ApiVersionsRequest`, a broker returns its full list of
    supported ApiKeys and versions regardless of current authentication
    state (e.g., before SASL authentication on an SASL listener, do note
    that no Kafka protocol requests may take place on an SSL listener
    before the SSL handshake is finished). If this is considered to leak
    information about the broker version a workaround is to use SSL with
    client authentication which is performed at an earlier stage of the
    connection where the `ApiVersionRequest` is not available. Also,
    note that broker versions older than 0.10.0.0 do not support this
    API and will either ignore the request or close connection in
    response to the request.
3.  If multiple versions of an API are supported by broker and client,
    clients are recommended to use the latest version supported by the
    broker and itself.
4.  Deprecation of a protocol version is done by marking an API version
    as deprecated in the protocol documentation.
5.  Supported API versions obtained from a broker are only valid for the
    connection on which that information is obtained. In the event of
    disconnection, the client should obtain the information from the
    broker again, as the broker might have been upgraded/downgraded in
    the mean time.

##### SASL Authentication Sequence {#sasl_handshake .anchor-link}

The following sequence is used for SASL authentication:

1.  Kafka `ApiVersionsRequest` may be sent by the client to obtain the
    version ranges of requests supported by the broker. This is
    optional.
2.  Kafka `SaslHandshakeRequest` containing the SASL mechanism for
    authentication is sent by the client. If the requested mechanism is
    not enabled in the server, the server responds with the list of
    supported mechanisms and closes the client connection. If the
    mechanism is enabled in the server, the server sends a successful
    response and continues with SASL authentication.
3.  The actual SASL authentication is now performed. If
    `SaslHandshakeRequest` version is v0, a series of SASL client and
    server tokens corresponding to the mechanism are sent as opaque
    packets without wrapping the messages with Kafka protocol headers.
    If `SaslHandshakeRequest` version is v1, the `SaslAuthenticate`
    request/response are used, where the actual SASL tokens are wrapped
    in the Kafka protocol. The error code in the final message from the
    broker will indicate if authentication succeeded or failed.
4.  If authentication succeeds, subsequent packets are handled as Kafka
    API requests. Otherwise, the client connection is closed.

For interoperability with 0.9.0.x clients, the first packet received by
the server is handled as a SASL/GSSAPI client token if it is not a valid
Kafka request. SASL/GSSAPI authentication is performed starting with
this packet, skipping the first two steps above.

#### The Protocol {#protocol_details .anchor-link}

##### Protocol Primitive Types {#protocol_types .anchor-link}

The protocol is built out of the following primitive types.

<!--#include virtual="generated/protocol_types.html" -->

##### Notes on reading the request format grammars {#protocol_grammar .anchor-link}

The [BNF](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_Form)s below
give an exact context free grammar for the request and response binary
format. The BNF is intentionally not compact in order to give
human-readable name. As always in a BNF a sequence of productions
indicates concatenation. When there are multiple possible productions
these are separated with \'\|\' and may be enclosed in parenthesis for
grouping. The top-level definition is always given first and subsequent
sub-parts are indented.

##### Common Request and Response Structure {#protocol_common .anchor-link}

All requests and responses originate from the following grammar which
will be incrementally describe through the rest of this document:

```
RequestOrResponse => Size (RequestMessage | ResponseMessage)
  Size => int32
```

| Field        | Description                                                                                                                                                                                                                                       |
|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| message_size | The message_size field gives the size of the subsequent request or response message in bytes. The client can read requests by first reading this 4 byte size as an integer N, and then reading and parsing the subsequent N bytes of the request. |

##### Record Batch {#protocol_recordbatch .anchor-link}

A description of the record batch format can be found
[here](/documentation/#recordbatch).

#### Constants {#protocol_constants .anchor-link}

##### Error Codes {#protocol_error_codes .anchor-link}

We use numeric codes to indicate what problem occurred on the server.
These can be translated by the client into exceptions or whatever the
appropriate error handling mechanism in the client language. Here is a
table of the error codes currently in use:

<!--#include virtual="generated/protocol_errors.html" -->

##### Api Keys {#protocol_api_keys .anchor-link}

The following are the numeric codes that the ApiKey in the request can
take for each of the below request types.

<!--#include virtual="generated/protocol_api_keys.html" -->

#### The Messages {#protocol_messages .anchor-link}

This section gives details on each of the individual API Messages, their
usage, their binary format, and the meaning of their fields.

<!--#include virtual="generated/protocol_messages.html" -->

#### Some Common Philosophical Questions {#protocol_philosophy .anchor-link}

Some people have asked why we don\'t use HTTP. There are a number of
reasons, the best is that client implementors can make use of some of
the more advanced TCP features\--the ability to multiplex requests, the
ability to simultaneously poll many connections, etc. We have also found
HTTP libraries in many languages to be surprisingly shabby.

Others have asked if maybe we shouldn\'t support many different
protocols. Prior experience with this was that it makes it very hard to
add and test new features if they have to be ported across many protocol
implementations. Our feeling is that most users don\'t really see
multiple protocols as a feature, they just want a good reliable client
in the language of their choice.

Another question is why we don\'t adopt XMPP, STOMP, AMQP or an existing
protocol. The answer to this varies by protocol, but in general the
problem is that the protocol does determine large parts of the
implementation and we couldn\'t do what we are doing if we didn\'t have
control over the protocol. Our belief is that it is possible to do
better than existing messaging systems have in providing a truly
distributed messaging system, and to do this we need to build something
that works differently.

A final question is why we don\'t use a system like Protocol Buffers or
Thrift to define our request messages. These packages excel at helping
you to managing lots and lots of serialized messages. However we have
only a few messages. Support across languages is somewhat spotty
(depending on the package). Finally the mapping between binary log
format and wire protocol is something we manage somewhat carefully and
this would not be possible with these systems. Finally we prefer the
style of versioning APIs explicitly and checking this to inferring new
values as nulls as it allows more nuanced control of compatibility.
