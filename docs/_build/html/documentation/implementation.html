

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Implementation &mdash; Apache Kafka 4.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Apache Kafka 4.0.0 documentation" href="../index.html"/>
        <link rel="up" title="Apache Kafka Documentation" href="index.html"/>
        <link rel="next" title="Operations" href="ops.html"/>
        <link rel="prev" title="Design" href="design.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Apache Kafka
          

          
          </a>

          
            
            
              <div class="version">
                4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uses.html">Use Cases</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Apache Kafka Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html">Kafka APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="design.html">Design</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#network-layer">5.1 Network Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#messages">5.2 Messages</a></li>
<li class="toctree-l3"><a class="reference internal" href="#message-format">5.3 Message Format</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#record-batch">5.3.1 Record Batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="#record">5.3.2 Record</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#log">5.4 Log</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#writes">Writes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reads">Reads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deletes">Deletes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#guarantees">Guarantees</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distribution">5.5 Distribution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#consumer-offset-tracking">Consumer Offset Tracking</a></li>
<li class="toctree-l4"><a class="reference internal" href="#zookeeper-directories">ZooKeeper Directories</a></li>
<li class="toctree-l4"><a class="reference internal" href="#notation">Notation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#broker-node-registry">Broker Node Registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="#broker-topic-registry">Broker Topic Registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="#consumers-and-consumer-groups">Consumers and Consumer Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="#consumer-id-registry">Consumer Id Registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="#consumer-offsets">Consumer Offsets</a></li>
<li class="toctree-l4"><a class="reference internal" href="#partition-owner-registry">Partition Owner registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-id">Cluster Id</a></li>
<li class="toctree-l4"><a class="reference internal" href="#broker-node-registration">Broker node registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#consumer-registration-algorithm">Consumer registration algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#consumer-rebalancing-algorithm">Consumer rebalancing algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ops.html">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l2"><a class="reference internal" href="connect.html">Kafka Connect</a></li>
<li class="toctree-l2"><a class="reference internal" href="streams/index.html">Kafka Streams</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Apache Kafka</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Apache Kafka Documentation</a> &raquo;</li>
        
      <li>Implementation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/documentation/implementation.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="implementation">
<span id="id1"></span><h1>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#network-layer" id="id2">5.1 Network Layer</a></li>
<li><a class="reference internal" href="#messages" id="id3">5.2 Messages</a></li>
<li><a class="reference internal" href="#message-format" id="id4">5.3 Message Format</a><ul>
<li><a class="reference internal" href="#record-batch" id="id5">5.3.1 Record Batch</a><ul>
<li><a class="reference internal" href="#control-batches" id="id6">5.3.1.1 Control Batches</a></li>
</ul>
</li>
<li><a class="reference internal" href="#record" id="id7">5.3.2 Record</a><ul>
<li><a class="reference internal" href="#record-header" id="id8">5.4.2.1 Record Header</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#log" id="id9">5.4 Log</a><ul>
<li><a class="reference internal" href="#writes" id="id10">Writes</a></li>
<li><a class="reference internal" href="#reads" id="id11">Reads</a></li>
<li><a class="reference internal" href="#deletes" id="id12">Deletes</a></li>
<li><a class="reference internal" href="#guarantees" id="id13">Guarantees</a></li>
</ul>
</li>
<li><a class="reference internal" href="#distribution" id="id14">5.5 Distribution</a><ul>
<li><a class="reference internal" href="#consumer-offset-tracking" id="id15">Consumer Offset Tracking</a><ul>
<li><a class="reference internal" href="#migrating-offsets-from-zookeeper-to-kafka" id="id16">Migrating offsets from ZooKeeper to Kafka</a></li>
</ul>
</li>
<li><a class="reference internal" href="#zookeeper-directories" id="id17">ZooKeeper Directories</a></li>
<li><a class="reference internal" href="#notation" id="id18">Notation</a></li>
<li><a class="reference internal" href="#broker-node-registry" id="id19">Broker Node Registry</a></li>
<li><a class="reference internal" href="#broker-topic-registry" id="id20">Broker Topic Registry</a></li>
<li><a class="reference internal" href="#consumers-and-consumer-groups" id="id21">Consumers and Consumer Groups</a></li>
<li><a class="reference internal" href="#consumer-id-registry" id="id22">Consumer Id Registry</a></li>
<li><a class="reference internal" href="#consumer-offsets" id="id23">Consumer Offsets</a></li>
<li><a class="reference internal" href="#partition-owner-registry" id="id24">Partition Owner registry</a></li>
<li><a class="reference internal" href="#cluster-id" id="id25">Cluster Id</a></li>
<li><a class="reference internal" href="#broker-node-registration" id="id26">Broker node registration</a></li>
<li><a class="reference internal" href="#consumer-registration-algorithm" id="id27">Consumer registration algorithm</a></li>
<li><a class="reference internal" href="#consumer-rebalancing-algorithm" id="id28">Consumer rebalancing algorithm</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="network-layer">
<h2><a class="reference external" href="#networklayer">5.1 Network Layer</a><a class="headerlink" href="#network-layer" title="Permalink to this headline">¶</a></h2>
<p>The network layer is a fairly straight-forward NIO server, and will not
be described in great detail. The sendfile implementation is done by
giving the <code class="docutils literal"><span class="pre">MessageSet</span></code> interface a <code class="docutils literal"><span class="pre">writeTo</span></code> method. This allows
the file-backed message set to use the more efficient <code class="docutils literal"><span class="pre">transferTo</span></code>
implementation instead of an in-process buffered write. The threading
model is a single acceptor thread and <em>N</em> processor threads which handle
a fixed number of connections each. This design has been pretty
thoroughly tested
<a class="reference external" href="http://sna-projects.com/blog/2009/08/introducing-the-nio-socketserver-implementation">elsewhere</a>
and found to be simple to implement and fast. The protocol is kept quite
simple to allow for future implementation of clients in other languages.</p>
</div>
<div class="section" id="messages">
<h2><a class="reference external" href="#messages">5.2 Messages</a><a class="headerlink" href="#messages" title="Permalink to this headline">¶</a></h2>
<p>Messages consist of a variable-length header, a variable length opaque
key byte array and a variable length opaque value byte array. The format
of the header is described in the following section. Leaving the key and
value opaque is the right decision: there is a great deal of progress
being made on serialization libraries right now, and any particular
choice is unlikely to be right for all uses. Needless to say a
particular application using Kafka would likely mandate a particular
serialization type as part of its usage. The <code class="docutils literal"><span class="pre">RecordBatch</span></code> interface
is simply an iterator over messages with specialized methods for bulk
reading and writing to an NIO <code class="docutils literal"><span class="pre">Channel</span></code>.</p>
</div>
<div class="section" id="message-format">
<h2><a class="reference external" href="#messageformat">5.3 Message Format</a><a class="headerlink" href="#message-format" title="Permalink to this headline">¶</a></h2>
<p>Messages (aka Records) are always written in batches. The technical term
for a batch of messages is a record batch, and a record batch contains
one or more records. In the degenerate case, we could have a record
batch containing a single record. Record batches and records have their
own headers. The format of each is described below for Kafka version
0.11.0 and later (message format version v2, or magic=2). <a class="reference external" href="https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Messagesets">Click
here</a>
for details about message formats 0 and 1.</p>
<div class="section" id="record-batch">
<h3><a class="reference external" href="#recordbatch">5.3.1 Record Batch</a><a class="headerlink" href="#record-batch" title="Permalink to this headline">¶</a></h3>
<p>The following is the on-disk format of a RecordBatch.</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">baseOffset</span><span class="p">:</span> <span class="n">int64</span>
<span class="n">batchLength</span><span class="p">:</span> <span class="n">int32</span>
<span class="n">partitionLeaderEpoch</span><span class="p">:</span> <span class="n">int32</span>
<span class="n">magic</span><span class="p">:</span> <span class="n">int8</span> <span class="p">(</span><span class="n">current</span> <span class="n">magic</span> <span class="n">value</span> <span class="ow">is</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">crc</span><span class="p">:</span> <span class="n">int32</span>
<span class="n">attributes</span><span class="p">:</span> <span class="n">int16</span>
    <span class="n">bit</span> <span class="mi">0</span><span class="o">~</span><span class="mi">2</span><span class="p">:</span>
        <span class="mi">0</span><span class="p">:</span> <span class="n">no</span> <span class="n">compression</span>
        <span class="mi">1</span><span class="p">:</span> <span class="n">gzip</span>
        <span class="mi">2</span><span class="p">:</span> <span class="n">snappy</span>
        <span class="mi">3</span><span class="p">:</span> <span class="n">lz4</span>
    <span class="n">bit</span> <span class="mi">3</span><span class="p">:</span> <span class="n">timestampType</span>
    <span class="n">bit</span> <span class="mi">4</span><span class="p">:</span> <span class="n">isTransactional</span> <span class="p">(</span><span class="mi">0</span> <span class="n">means</span> <span class="ow">not</span> <span class="n">transactional</span><span class="p">)</span>
    <span class="n">bit</span> <span class="mi">5</span><span class="p">:</span> <span class="n">isControlBatch</span> <span class="p">(</span><span class="mi">0</span> <span class="n">means</span> <span class="ow">not</span> <span class="n">a</span> <span class="n">control</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">bit</span> <span class="mi">6</span><span class="o">~</span><span class="mi">15</span><span class="p">:</span> <span class="n">unused</span>
<span class="n">lastOffsetDelta</span><span class="p">:</span> <span class="n">int32</span>
<span class="n">firstTimestamp</span><span class="p">:</span> <span class="n">int64</span>
<span class="n">maxTimestamp</span><span class="p">:</span> <span class="n">int64</span>
<span class="n">producerId</span><span class="p">:</span> <span class="n">int64</span>
<span class="n">producerEpoch</span><span class="p">:</span> <span class="n">int16</span>
<span class="n">baseSequence</span><span class="p">:</span> <span class="n">int32</span>
<span class="n">records</span><span class="p">:</span> <span class="p">[</span><span class="n">Record</span><span class="p">]</span>
</pre></div>
</div>
<p>Note that when compression is enabled, the compressed record data is
serialized directly following the count of the number of records.</p>
<p>The CRC covers the data from the attributes to the end of the batch
(i.e. all the bytes that follow the CRC). It is located after the magic
byte, which means that clients must parse the magic byte before deciding
how to interpret the bytes between the batch length and the magic byte.
The partition leader epoch field is not included in the CRC computation
to avoid the need to recompute the CRC when this field is assigned for
every batch that is received by the broker. The CRC-32C (Castagnoli)
polynomial is used for the computation.</p>
<p>On compaction: unlike the older message formats, magic v2 and above
preserves the first and last offset/sequence numbers from the original
batch when the log is cleaned. This is required in order to be able to
restore the producer&#8217;s state when the log is reloaded. If we did not
retain the last sequence number, for example, then after a partition
leader failure, the producer might see an OutOfSequence error. The base
sequence number must be preserved for duplicate checking (the broker
checks incoming Produce requests for duplicates by verifying that the
first and last sequence numbers of the incoming batch match the last
from that producer). As a result, it is possible to have empty batches
in the log when all the records in the batch are cleaned but batch is
still retained in order to preserve a producer&#8217;s last sequence number.
One oddity here is that the baseTimestamp field is not preserved during
compaction, so it will change if the first record in the batch is
compacted away.</p>
<div class="section" id="control-batches">
<h4><a class="reference external" href="#controlbatch">5.3.1.1 Control Batches</a><a class="headerlink" href="#control-batches" title="Permalink to this headline">¶</a></h4>
<p>A control batch contains a single record called the control record.
Control records should not be passed on to applications. Instead, they
are used by consumers to filter out aborted transactional messages.</p>
<p>The key of a control record conforms to the following schema:</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">version</span><span class="p">:</span> <span class="n">int16</span> <span class="p">(</span><span class="n">current</span> <span class="n">version</span> <span class="ow">is</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">type</span><span class="p">:</span> <span class="n">int16</span> <span class="p">(</span><span class="mi">0</span> <span class="n">indicates</span> <span class="n">an</span> <span class="n">abort</span> <span class="n">marker</span><span class="p">,</span> <span class="mi">1</span> <span class="n">indicates</span> <span class="n">a</span> <span class="n">commit</span><span class="p">)</span>
</pre></div>
</div>
<p>The schema for the value of a control record is dependent on the type.
The value is opaque to clients.</p>
</div>
</div>
<div class="section" id="record">
<h3><a class="reference external" href="#record">5.3.2 Record</a><a class="headerlink" href="#record" title="Permalink to this headline">¶</a></h3>
<p>Record level headers were introduced in Kafka 0.11.0. The on-disk format
of a record with Headers is delineated below.</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">length</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">attributes</span><span class="p">:</span> <span class="n">int8</span>
    <span class="n">bit</span> <span class="mi">0</span><span class="o">~</span><span class="mi">7</span><span class="p">:</span> <span class="n">unused</span>
<span class="n">timestampDelta</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">offsetDelta</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">keyLength</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">key</span><span class="p">:</span> <span class="n">byte</span><span class="p">[]</span>
<span class="n">valueLen</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">value</span><span class="p">:</span> <span class="n">byte</span><span class="p">[]</span>
<span class="n">Headers</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">Header</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="record-header">
<h4><a class="reference external" href="#recordheader">5.4.2.1 Record Header</a><a class="headerlink" href="#record-header" title="Permalink to this headline">¶</a></h4>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">headerKeyLength</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">headerKey</span><span class="p">:</span> <span class="n">String</span>
<span class="n">headerValueLength</span><span class="p">:</span> <span class="n">varint</span>
<span class="n">Value</span><span class="p">:</span> <span class="n">byte</span><span class="p">[]</span>
</pre></div>
</div>
<p>We use the same varint encoding as Protobuf. More information on the
latter can be found
<a class="reference external" href="https://developers.google.com/protocol-buffers/docs/encoding#varints">here</a>.
The count of headers in a record is also encoded as a varint.</p>
</div>
</div>
</div>
<div class="section" id="log">
<h2><a class="reference external" href="#log">5.4 Log</a><a class="headerlink" href="#log" title="Permalink to this headline">¶</a></h2>
<p>A log for a topic named &#8220;my_topic&#8221; with two partitions consists of two
directories (namely <code class="docutils literal"><span class="pre">my_topic_0</span></code> and <code class="docutils literal"><span class="pre">my_topic_1</span></code>) populated with
data files containing the messages for that topic. The format of the log
files is a sequence of &#8220;log entries&#8221;&#8221;; each log entry is a 4 byte
integer <em>N</em> storing the message length which is followed by the <em>N</em>
message bytes. Each message is uniquely identified by a 64-bit integer
<em>offset</em> giving the byte position of the start of this message in the
stream of all messages ever sent to that topic on that partition. The
on-disk format of each message is given below. Each log file is named
with the offset of the first message it contains. So the first file
created will be 00000000000.kafka, and each additional file will have an
integer name roughly <em>S</em> bytes from the previous file where <em>S</em> is the
max log file size given in the configuration.</p>
<p>The exact binary format for records is versioned and maintained as a
standard interface so record batches can be transferred between
producer, broker, and client without recopying or conversion when
desirable. The previous section included details about the on-disk
format of records.</p>
<p>The use of the message offset as the message id is unusual. Our original
idea was to use a GUID generated by the producer, and maintain a mapping
from GUID to offset on each broker. But since a consumer must maintain
an ID for each server, the global uniqueness of the GUID provides no
value. Furthermore, the complexity of maintaining the mapping from a
random id to an offset requires a heavy weight index structure which
must be synchronized with disk, essentially requiring a full persistent
random-access data structure. Thus to simplify the lookup structure we
decided to use a simple per-partition atomic counter which could be
coupled with the partition id and node id to uniquely identify a
message; this makes the lookup structure simpler, though multiple seeks
per consumer request are still likely. However once we settled on a
counter, the jump to directly using the offset seemed natural—both after
all are monotonically increasing integers unique to a partition. Since
the offset is hidden from the consumer API this decision is ultimately
an implementation detail and we went with the more efficient approach.</p>
<img alt="../_images/kafka_log.png" src="../_images/kafka_log.png" />
<div class="section" id="writes">
<h3><a class="reference external" href="#impl_writes">Writes</a><a class="headerlink" href="#writes" title="Permalink to this headline">¶</a></h3>
<p>The log allows serial appends which always go to the last file. This
file is rolled over to a fresh file when it reaches a configurable size
(say 1GB). The log takes two configuration parameters: <em>M</em>, which gives
the number of messages to write before forcing the OS to flush the file
to disk, and <em>S</em>, which gives a number of seconds after which a flush is
forced. This gives a durability guarantee of losing at most <em>M</em> messages
or <em>S</em> seconds of data in the event of a system crash.</p>
</div>
<div class="section" id="reads">
<h3><a class="reference external" href="#impl_reads">Reads</a><a class="headerlink" href="#reads" title="Permalink to this headline">¶</a></h3>
<p>Reads are done by giving the 64-bit logical offset of a message and an
<em>S</em>-byte max chunk size. This will return an iterator over the messages
contained in the <em>S</em>-byte buffer. <em>S</em> is intended to be larger than any
single message, but in the event of an abnormally large message, the
read can be retried multiple times, each time doubling the buffer size,
until the message is read successfully. A maximum message and buffer
size can be specified to make the server reject messages larger than
some size, and to give a bound to the client on the maximum it needs to
ever read to get a complete message. It is likely that the read buffer
ends with a partial message, this is easily detected by the size
delimiting.</p>
<p>The actual process of reading from an offset requires first locating the
log segment file in which the data is stored, calculating the
file-specific offset from the global offset value, and then reading from
that file offset. The search is done as a simple binary search variation
against an in-memory range maintained for each file.</p>
<p>The log provides the capability of getting the most recently written
message to allow clients to start subscribing as of &#8220;right now&#8221;. This is
also useful in the case the consumer fails to consume its data within
its SLA-specified number of days. In this case when the client attempts
to consume a non-existent offset it is given an OutOfRangeException and
can either reset itself or fail as appropriate to the use case.</p>
<p>The following is the format of the results sent to the consumer.</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">MessageSetSend</span> <span class="p">(</span><span class="n">fetch</span> <span class="n">result</span><span class="p">)</span>

<span class="n">total</span> <span class="n">length</span>     <span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span>
<span class="n">error</span> <span class="n">code</span>       <span class="p">:</span> <span class="mi">2</span> <span class="nb">bytes</span>
<span class="n">message</span> <span class="mi">1</span>        <span class="p">:</span> <span class="n">x</span> <span class="nb">bytes</span>
<span class="o">...</span>
<span class="n">message</span> <span class="n">n</span>        <span class="p">:</span> <span class="n">x</span> <span class="nb">bytes</span>
</pre></div>
</div>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">MultiMessageSetSend</span> <span class="p">(</span><span class="n">multiFetch</span> <span class="n">result</span><span class="p">)</span>

<span class="n">total</span> <span class="n">length</span>       <span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span>
<span class="n">error</span> <span class="n">code</span>         <span class="p">:</span> <span class="mi">2</span> <span class="nb">bytes</span>
<span class="n">messageSetSend</span> <span class="mi">1</span>
<span class="o">...</span>
<span class="n">messageSetSend</span> <span class="n">n</span>
</pre></div>
</div>
</div>
<div class="section" id="deletes">
<h3><a class="reference external" href="#impl_deletes">Deletes</a><a class="headerlink" href="#deletes" title="Permalink to this headline">¶</a></h3>
<p>Data is deleted one log segment at a time. The log manager allows
pluggable delete policies to choose which files are eligible for
deletion. The current policy deletes any log with a modification time of
more than <em>N</em> days ago, though a policy which retained the last <em>N</em> GB
could also be useful. To avoid locking reads while still allowing
deletes that modify the segment list we use a copy-on-write style
segment list implementation that provides consistent views to allow a
binary search to proceed on an immutable static snapshot view of the log
segments while deletes are progressing.</p>
</div>
<div class="section" id="guarantees">
<h3><a class="reference external" href="#impl_guarantees">Guarantees</a><a class="headerlink" href="#guarantees" title="Permalink to this headline">¶</a></h3>
<p>The log provides a configuration parameter <em>M</em> which controls the
maximum number of messages that are written before forcing a flush to
disk. On startup a log recovery process is run that iterates over all
messages in the newest log segment and verifies that each message entry
is valid. A message entry is valid if the sum of its size and offset are
less than the length of the file AND the CRC32 of the message payload
matches the CRC stored with the message. In the event corruption is
detected the log is truncated to the last valid offset.</p>
<p>Note that two kinds of corruption must be handled: truncation in which
an unwritten block is lost due to a crash, and corruption in which a
nonsense block is ADDED to the file. The reason for this is that in
general the OS makes no guarantee of the write order between the file
inode and the actual block data so in addition to losing written data
the file can gain nonsense data if the inode is updated with a new size
but a crash occurs before the block containing that data is written. The
CRC detects this corner case, and prevents it from corrupting the log
(though the unwritten messages are, of course, lost).</p>
</div>
</div>
<div class="section" id="distribution">
<h2><a class="reference external" href="#distributionimpl">5.5 Distribution</a><a class="headerlink" href="#distribution" title="Permalink to this headline">¶</a></h2>
<div class="section" id="consumer-offset-tracking">
<h3><a class="reference external" href="#impl_offsettracking">Consumer Offset Tracking</a><a class="headerlink" href="#consumer-offset-tracking" title="Permalink to this headline">¶</a></h3>
<p>The high-level consumer tracks the maximum offset it has consumed in
each partition and periodically commits its offset vector so that it can
resume from those offsets in the event of a restart. Kafka provides the
option to store all the offsets for a given consumer group in a
designated broker (for that group) called the <em>offset manager</em>. i.e.,
any consumer instance in that consumer group should send its offset
commits and fetches to that offset manager (broker). The high-level
consumer handles this automatically. If you use the simple consumer you
will need to manage offsets manually. This is currently unsupported in
the Java simple consumer which can only commit or fetch offsets in
ZooKeeper. If you use the Scala simple consumer you can discover the
offset manager and explicitly commit or fetch offsets to the offset
manager. A consumer can look up its offset manager by issuing a
GroupCoordinatorRequest to any Kafka broker and reading the
GroupCoordinatorResponse which will contain the offset manager. The
consumer can then proceed to commit or fetch offsets from the offsets
manager broker. In case the offset manager moves, the consumer will need
to rediscover the offset manager. If you wish to manage your offsets
manually, you can take a look at these <a class="reference external" href="https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka">code samples that explain how to
issue OffsetCommitRequest and
OffsetFetchRequest</a>.</p>
<p>When the offset manager receives an OffsetCommitRequest, it appends the
request to a special <a class="reference external" href="#compaction">compacted</a> Kafka topic named
<em>__consumer_offsets</em>. The offset manager sends a successful offset
commit response to the consumer only after all the replicas of the
offsets topic receive the offsets. In case the offsets fail to replicate
within a configurable timeout, the offset commit will fail and the
consumer may retry the commit after backing off. (This is done
automatically by the high-level consumer.) The brokers periodically
compact the offsets topic since it only needs to maintain the most
recent offset commit per partition. The offset manager also caches the
offsets in an in-memory table in order to serve offset fetches quickly.</p>
<p>When the offset manager receives an offset fetch request, it simply
returns the last committed offset vector from the offsets cache. In case
the offset manager was just started or if it just became the offset
manager for a new set of consumer groups (by becoming a leader for a
partition of the offsets topic), it may need to load the offsets topic
partition into the cache. In this case, the offset fetch will fail with
an OffsetsLoadInProgress exception and the consumer may retry the
OffsetFetchRequest after backing off. (This is done automatically by the
high-level consumer.)</p>
<div class="section" id="migrating-offsets-from-zookeeper-to-kafka">
<h4><a class="reference external" href="#offsetmigration">Migrating offsets from ZooKeeper to Kafka</a><a class="headerlink" href="#migrating-offsets-from-zookeeper-to-kafka" title="Permalink to this headline">¶</a></h4>
<p>Kafka consumers in earlier releases store their offsets by default in
ZooKeeper. It is possible to migrate these consumers to commit offsets
into Kafka by following these steps:</p>
<ol class="arabic simple">
<li>Set <code class="docutils literal"><span class="pre">offsets.storage=kafka</span></code> and <code class="docutils literal"><span class="pre">dual.commit.enabled=true</span></code> in
your consumer config.</li>
<li>Do a rolling bounce of your consumers and then verify that your
consumers are healthy.</li>
<li>Set <code class="docutils literal"><span class="pre">dual.commit.enabled=false</span></code> in your consumer config.</li>
<li>Do a rolling bounce of your consumers and then verify that your
consumers are healthy.</li>
</ol>
<p>A roll-back (i.e., migrating from Kafka back to ZooKeeper) can also be
performed using the above steps if you set
<code class="docutils literal"><span class="pre">offsets.storage=zookeeper</span></code>.</p>
</div>
</div>
<div class="section" id="zookeeper-directories">
<h3><a class="reference external" href="#impl_zookeeper">ZooKeeper Directories</a><a class="headerlink" href="#zookeeper-directories" title="Permalink to this headline">¶</a></h3>
<p>The following gives the ZooKeeper structures and algorithms used for
co-ordination between consumers and brokers.</p>
</div>
<div class="section" id="notation">
<h3><a class="reference external" href="#impl_zknotation">Notation</a><a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h3>
<p>When an element in a path is denoted [xyz], that means that the value of
xyz is not fixed and there is in fact a ZooKeeper znode for each
possible value of xyz. For example /topics/[topic] would be a directory
named /topics containing a sub-directory for each topic name. Numerical
ranges are also given such as [0...5] to indicate the subdirectories 0,
1, 2, 3, 4. An arrow -&gt; is used to indicate the contents of a znode. For
example /hello -&gt; world would indicate a znode /hello containing the
value &#8220;world&#8221;.</p>
</div>
<div class="section" id="broker-node-registry">
<h3><a class="reference external" href="#impl_zkbroker">Broker Node Registry</a><a class="headerlink" href="#broker-node-registry" title="Permalink to this headline">¶</a></h3>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">brokers</span><span class="o">/</span><span class="n">ids</span><span class="o">/</span><span class="p">[</span><span class="mf">0.</span><span class="o">..</span><span class="n">N</span><span class="p">]</span> <span class="o">--&gt;</span> <span class="p">{</span><span class="s2">&quot;jmx_port&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;timestamp&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;endpoints&quot;</span><span class="p">:[</span><span class="o">...</span><span class="p">],</span><span class="s2">&quot;host&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;version&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;port&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">}</span> <span class="p">(</span><span class="n">ephemeral</span> <span class="n">node</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a list of all present broker nodes, each of which provides a
unique logical broker id which identifies it to consumers (which must be
given as part of its configuration). On startup, a broker node registers
itself by creating a znode with the logical broker id under
/brokers/ids. The purpose of the logical broker id is to allow a broker
to be moved to a different physical machine without affecting consumers.
An attempt to register a broker id that is already in use (say because
two servers are configured with the same broker id) results in an error.</p>
<p>Since the broker registers itself in ZooKeeper using ephemeral znodes,
this registration is dynamic and will disappear if the broker is
shutdown or dies (thus notifying consumers it is no longer available).</p>
</div>
<div class="section" id="broker-topic-registry">
<h3><a class="reference external" href="#impl_zktopic">Broker Topic Registry</a><a class="headerlink" href="#broker-topic-registry" title="Permalink to this headline">¶</a></h3>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">brokers</span><span class="o">/</span><span class="n">topics</span><span class="o">/</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span><span class="o">/</span><span class="n">partitions</span><span class="o">/</span><span class="p">[</span><span class="mf">0.</span><span class="o">..</span><span class="n">N</span><span class="p">]</span><span class="o">/</span><span class="n">state</span> <span class="o">--&gt;</span> <span class="p">{</span><span class="s2">&quot;controller_epoch&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;leader&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;version&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;leader_epoch&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;isr&quot;</span><span class="p">:[</span><span class="o">...</span><span class="p">]}</span> <span class="p">(</span><span class="n">ephemeral</span> <span class="n">node</span><span class="p">)</span>
</pre></div>
</div>
<p>Each broker registers itself under the topics it maintains and stores
the number of partitions for that topic.</p>
</div>
<div class="section" id="consumers-and-consumer-groups">
<h3><a class="reference external" href="#impl_zkconsumers">Consumers and Consumer Groups</a><a class="headerlink" href="#consumers-and-consumer-groups" title="Permalink to this headline">¶</a></h3>
<p>Consumers of topics also register themselves in ZooKeeper, in order to
coordinate with each other and balance the consumption of data.
Consumers can also store their offsets in ZooKeeper by setting
<code class="docutils literal"><span class="pre">offsets.storage=zookeeper</span></code>. However, this offset storage mechanism
will be deprecated in a future release. Therefore, it is recommended to
<a class="reference external" href="#offsetmigration">migrate offsets storage to Kafka</a>.</p>
<p>Multiple consumers can form a group and jointly consume a single topic.
Each consumer in the same group is given a shared group_id. For example
if one consumer is your foobar process, which is run across three
machines, then you might assign this group of consumers the id &#8220;foobar&#8221;.
This group id is provided in the configuration of the consumer, and is
your way to tell the consumer which group it belongs to.</p>
<p>The consumers in a group divide up the partitions as fairly as possible,
each partition is consumed by exactly one consumer in a consumer group.</p>
</div>
<div class="section" id="consumer-id-registry">
<h3><a class="reference external" href="#impl_zkconsumerid">Consumer Id Registry</a><a class="headerlink" href="#consumer-id-registry" title="Permalink to this headline">¶</a></h3>
<p>In addition to the group_id which is shared by all consumers in a group,
each consumer is given a transient, unique consumer_id (of the form
hostname:uuid) for identification purposes. Consumer ids are registered
in the following directory.</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">consumers</span><span class="o">/</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span><span class="o">/</span><span class="n">ids</span><span class="o">/</span><span class="p">[</span><span class="n">consumer_id</span><span class="p">]</span> <span class="o">--&gt;</span> <span class="p">{</span><span class="s2">&quot;version&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;subscription&quot;</span><span class="p">:{</span><span class="o">...</span><span class="p">:</span><span class="o">...</span><span class="p">},</span><span class="s2">&quot;pattern&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span><span class="s2">&quot;timestamp&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">}</span> <span class="p">(</span><span class="n">ephemeral</span> <span class="n">node</span><span class="p">)</span>
</pre></div>
</div>
<p>Each of the consumers in the group registers under its group and creates
a znode with its consumer_id. The value of the znode contains a map of
&lt;topic, #streams&gt;. This id is simply used to identify each of the
consumers which is currently active within a group. This is an ephemeral
node so it will disappear if the consumer process dies.</p>
</div>
<div class="section" id="consumer-offsets">
<h3><a class="reference external" href="#impl_zkconsumeroffsets">Consumer Offsets</a><a class="headerlink" href="#consumer-offsets" title="Permalink to this headline">¶</a></h3>
<p>Consumers track the maximum offset they have consumed in each partition.
This value is stored in a ZooKeeper directory if
<code class="docutils literal"><span class="pre">offsets.storage=zookeeper</span></code>.</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">consumers</span><span class="o">/</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span><span class="o">/</span><span class="n">offsets</span><span class="o">/</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span><span class="o">/</span><span class="p">[</span><span class="n">partition_id</span><span class="p">]</span> <span class="o">--&gt;</span> <span class="n">offset_counter_value</span> <span class="p">(</span><span class="n">persistent</span> <span class="n">node</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="partition-owner-registry">
<h3><a class="reference external" href="#impl_zkowner">Partition Owner registry</a><a class="headerlink" href="#partition-owner-registry" title="Permalink to this headline">¶</a></h3>
<p>Each broker partition is consumed by a single consumer within a given
consumer group. The consumer must establish its ownership of a given
partition before any consumption can begin. To establish its ownership,
a consumer writes its own id in an ephemeral node under the particular
broker partition it is claiming.</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">consumers</span><span class="o">/</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span><span class="o">/</span><span class="n">owners</span><span class="o">/</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span><span class="o">/</span><span class="p">[</span><span class="n">partition_id</span><span class="p">]</span> <span class="o">--&gt;</span> <span class="n">consumer_node_id</span> <span class="p">(</span><span class="n">ephemeral</span> <span class="n">node</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="cluster-id">
<h3><a class="reference external" href="#impl_clusterid">Cluster Id</a><a class="headerlink" href="#cluster-id" title="Permalink to this headline">¶</a></h3>
<p>The cluster id is a unique and immutable identifier assigned to a Kafka
cluster. The cluster id can have a maximum of 22 characters and the
allowed characters are defined by the regular expression
[<a href="#id29"><span class="problematic" id="id30">a-zA-Z0-9_</span></a>-]+, which corresponds to the characters used by the
URL-safe Base64 variant with no padding. Conceptually, it is
auto-generated when a cluster is started for the first time.</p>
<p>Implementation-wise, it is generated when a broker with version 0.10.1
or later is successfully started for the first time. The broker tries to
get the cluster id from the <code class="docutils literal"><span class="pre">/cluster/id</span></code> znode during startup. If the
znode does not exist, the broker generates a new cluster id and creates
the znode with this cluster id.</p>
</div>
<div class="section" id="broker-node-registration">
<h3><a class="reference external" href="#impl_brokerregistration">Broker node registration</a><a class="headerlink" href="#broker-node-registration" title="Permalink to this headline">¶</a></h3>
<p>The broker nodes are basically independent, so they only publish
information about what they have. When a broker joins, it registers
itself under the broker node registry directory and writes information
about its host name and port. The broker also register the list of
existing topics and their logical partitions in the broker topic
registry. New topics are registered dynamically when they are created on
the broker.</p>
</div>
<div class="section" id="consumer-registration-algorithm">
<h3><a class="reference external" href="#impl_consumerregistration">Consumer registration algorithm</a><a class="headerlink" href="#consumer-registration-algorithm" title="Permalink to this headline">¶</a></h3>
<p>When a consumer starts, it does the following:</p>
<ol class="arabic simple">
<li>Register itself in the consumer id registry under its group.</li>
<li>Register a watch on changes (new consumers joining or any existing
consumers leaving) under the consumer id registry. (Each change
triggers rebalancing among all consumers within the group to which
the changed consumer belongs.)</li>
<li>Register a watch on changes (new brokers joining or any existing
brokers leaving) under the broker id registry. (Each change triggers
rebalancing among all consumers in all consumer groups.)</li>
<li>If the consumer creates a message stream using a topic filter, it
also registers a watch on changes (new topics being added) under the
broker topic registry. (Each change will trigger re-evaluation of the
available topics to determine which topics are allowed by the topic
filter. A new allowed topic will trigger rebalancing among all
consumers within the consumer group.)</li>
<li>Force itself to rebalance within in its consumer group.</li>
</ol>
</div>
<div class="section" id="consumer-rebalancing-algorithm">
<h3><a class="reference external" href="#impl_consumerrebalance">Consumer rebalancing algorithm</a><a class="headerlink" href="#consumer-rebalancing-algorithm" title="Permalink to this headline">¶</a></h3>
<p>The consumer rebalancing algorithms allows all the consumers in a group
to come into consensus on which consumer is consuming which partitions.
Consumer rebalancing is triggered on each addition or removal of both
broker nodes and other consumers within the same group. For a given
topic and a given consumer group, broker partitions are divided evenly
among consumers within the group. A partition is always consumed by a
single consumer. This design simplifies the implementation. Had we
allowed a partition to be concurrently consumed by multiple consumers,
there would be contention on the partition and some kind of locking
would be required. If there are more consumers than partitions, some
consumers won&#8217;t get any data at all. During rebalancing, we try to
assign partitions to consumers in such a way that reduces the number of
broker nodes each consumer has to connect to.</p>
<p>Each consumer does the following during rebalancing:</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">For</span> <span class="n">each</span> <span class="n">topic</span> <span class="n">T</span> <span class="n">that</span> <span class="n">Ci</span> <span class="n">subscribes</span> <span class="n">to</span>
<span class="mf">2.</span>   <span class="n">let</span> <span class="n">PT</span> <span class="n">be</span> <span class="nb">all</span> <span class="n">partitions</span> <span class="n">producing</span> <span class="n">topic</span> <span class="n">T</span>
<span class="mf">3.</span>   <span class="n">let</span> <span class="n">CG</span> <span class="n">be</span> <span class="nb">all</span> <span class="n">consumers</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">same</span> <span class="n">group</span> <span class="k">as</span> <span class="n">Ci</span> <span class="n">that</span> <span class="n">consume</span> <span class="n">topic</span> <span class="n">T</span>
<span class="mf">4.</span>   <span class="n">sort</span> <span class="n">PT</span> <span class="p">(</span><span class="n">so</span> <span class="n">partitions</span> <span class="n">on</span> <span class="n">the</span> <span class="n">same</span> <span class="n">broker</span> <span class="n">are</span> <span class="n">clustered</span> <span class="n">together</span><span class="p">)</span>
<span class="mf">5.</span>   <span class="n">sort</span> <span class="n">CG</span>
<span class="mf">6.</span>   <span class="n">let</span> <span class="n">i</span> <span class="n">be</span> <span class="n">the</span> <span class="n">index</span> <span class="n">position</span> <span class="n">of</span> <span class="n">Ci</span> <span class="ow">in</span> <span class="n">CG</span> <span class="ow">and</span> <span class="n">let</span> <span class="n">N</span> <span class="o">=</span> <span class="n">size</span><span class="p">(</span><span class="n">PT</span><span class="p">)</span><span class="o">/</span><span class="n">size</span><span class="p">(</span><span class="n">CG</span><span class="p">)</span>
<span class="mf">7.</span>   <span class="n">assign</span> <span class="n">partitions</span> <span class="kn">from</span> <span class="nn">i</span><span class="o">*</span><span class="n">N</span> <span class="n">to</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">consumer</span> <span class="n">Ci</span>
<span class="mf">8.</span>   <span class="n">remove</span> <span class="n">current</span> <span class="n">entries</span> <span class="n">owned</span> <span class="n">by</span> <span class="n">Ci</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">partition</span> <span class="n">owner</span> <span class="n">registry</span>
<span class="mf">9.</span>   <span class="n">add</span> <span class="n">newly</span> <span class="n">assigned</span> <span class="n">partitions</span> <span class="n">to</span> <span class="n">the</span> <span class="n">partition</span> <span class="n">owner</span> <span class="n">registry</span>
        <span class="p">(</span><span class="n">we</span> <span class="n">may</span> <span class="n">need</span> <span class="n">to</span> <span class="n">re</span><span class="o">-</span><span class="k">try</span> <span class="n">this</span> <span class="n">until</span> <span class="n">the</span> <span class="n">original</span> <span class="n">partition</span> <span class="n">owner</span> <span class="n">releases</span> <span class="n">its</span> <span class="n">ownership</span><span class="p">)</span>
</pre></div>
</div>
<p>When rebalancing is triggered at one consumer, rebalancing should be
triggered in other consumers within the same group about the same time.</p>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ops.html" class="btn btn-neutral float-right" title="Operations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="design.html" class="btn btn-neutral" title="Design" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Apache Software Foundation.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'4.0.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: ''
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>